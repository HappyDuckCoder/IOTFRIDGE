import os
import torch
import sounddevice as sd
import scipy.io.wavfile as wavfile
import numpy as np
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, GenerationConfig

# === Class xá»­ lÃ½ ghi Ã¢m ===
class Recorder:
    def __init__(self, duration=5, fs=16000, save_path="resources/audio/audio.wav"):
        self.duration = duration
        self.fs = fs
        self.save_path = save_path

    def record(self):
        print(f"ğŸ™ï¸ Báº¯t Ä‘áº§u ghi Ã¢m {self.duration} giÃ¢y...")
        recording = sd.rec(int(self.duration * self.fs), samplerate=self.fs, channels=1, dtype='float32')
        sd.wait()
        print("âœ… Ghi Ã¢m hoÃ n táº¥t.")
        return recording

    def save(self, data):
        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
        audio_int16 = np.int16(data * 32767)
        wavfile.write(self.save_path, self.fs, audio_int16)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o: {self.save_path}")

    def record_to_file(self):
        audio = self.record()
        self.save(audio)

# === Class xá»­ lÃ½ mÃ´ hÃ¬nh Ã¢m thanh ===
class AudioModel:
    def __init__(self, model_name="tranviethuy01/whisper-medium-vi", max_length=100):
        self.model_name = model_name
        self.max_length = max_length
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = None
        self.model = None

    def load(self):
        print("ğŸ§  Äang táº£i mÃ´ hÃ¬nh Whisper...")
        self.processor = AutoProcessor.from_pretrained(self.model_name)
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(self.model_name, torch_dtype=torch.float32)
        self.model.to(self.device)

        self.model.generation_config = GenerationConfig(
            max_length=self.max_length,
            pad_token_id=self.processor.tokenizer.pad_token_id,
            eos_token_id=self.processor.tokenizer.eos_token_id,
            decoder_start_token_id=self.model.config.decoder_start_token_id,
            use_cache=False
        )

    def transcribe(self, wav_path):
        print(f"ğŸ“‚ Äang Ä‘á»c file: {wav_path}")
        sr, audio = wavfile.read(wav_path)
        if audio.dtype == np.int16:
            audio = audio.astype(np.float32) / 32767.0

        audio_input = audio.flatten()
        inputs = self.processor(audio_input, sampling_rate=sr, return_tensors="pt")
        input_features = inputs["input_features"].to(self.device)
        attention_mask = torch.ones(input_features.shape[:-1], dtype=torch.long).to(self.device)

        print("ğŸ—£ï¸ Äang nháº­n diá»‡n giá»ng nÃ³i...")
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_features=input_features,
                attention_mask=attention_mask
            )

        return self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

# === HÃ m main ===
def main():
    recorder = Recorder()
    model = AudioModel()
    model.load()

    while True:
        cmd = input("\nNháº­p 'ok' Ä‘á»ƒ ghi Ã¢m (hoáº·c 'q' Ä‘á»ƒ thoÃ¡t): ").strip().lower()
        if cmd == "ok":
            recorder.record_to_file()
            text = model.transcribe(recorder.save_path)
            print("\nğŸ“„ Káº¿t quáº£ nháº­n diá»‡n:")
            print(text)
        elif cmd == "q":
            print("ğŸ‘‹ ThoÃ¡t chÆ°Æ¡ng trÃ¬nh.")
            break
        else:
            print("â— Vui lÃ²ng nháº­p 'ok' Ä‘á»ƒ ghi Ã¢m hoáº·c 'q' Ä‘á»ƒ thoÃ¡t.")

if __name__ == "__main__":
    main()
