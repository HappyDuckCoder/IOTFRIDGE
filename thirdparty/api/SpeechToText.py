import os
import torch
import sounddevice as sd
import scipy.io.wavfile as wavfile
import numpy as np
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, GenerationConfig

# === Class x·ª≠ l√Ω ghi √¢m ===
class Recorder:
    def __init__(self, duration=5, fs=16000, save_path="resources/audio/audio.wav"):
        self.duration = duration
        self.fs = fs
        self.save_path = save_path

    def record(self):
        print(f"B·∫Øt ƒë·∫ßu ghi √¢m {self.duration} gi√¢y...")
        recording = sd.rec(int(self.duration * self.fs), samplerate=self.fs, channels=1, dtype='float32')
        sd.wait()
        print("Ghi √¢m ho√†n t·∫•t.")
        return recording

    def save(self, data):
        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
        audio_int16 = np.int16(data * 32767)
        wavfile.write(self.save_path, self.fs, audio_int16)
        print(f"üíæ ƒê√£ l∆∞u v√†o: {self.save_path}")

    def record_to_file(self):
        audio = self.record()
        self.save(audio)

# === Class x·ª≠ l√Ω m√¥ h√¨nh √¢m thanh ===
class AudioModel:
    def __init__(self, model_name="tranviethuy01/whisper-medium-vi", max_length=100):
        self.model_name = model_name
        self.max_length = max_length
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = None
        self.model = None

    def load(self):
        print("ƒêang t·∫£i m√¥ h√¨nh Whisper...")
        self.processor = AutoProcessor.from_pretrained(self.model_name)
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(self.model_name, torch_dtype=torch.float32)
        self.model.to(self.device)

        self.model.generation_config = GenerationConfig(
            max_length=self.max_length,
            pad_token_id=self.processor.tokenizer.pad_token_id,
            eos_token_id=self.processor.tokenizer.eos_token_id,
            decoder_start_token_id=self.model.config.decoder_start_token_id,
            use_cache=False
        )

    def transcribe(self, wav_path):
        print(f"ƒêang ƒë·ªçc file: {wav_path}")
        sr, audio = wavfile.read(wav_path)
        if audio.dtype == np.int16:
            audio = audio.astype(np.float32) / 32767.0

        audio_input = audio.flatten()
        inputs = self.processor(audio_input, sampling_rate=sr, return_tensors="pt")
        input_features = inputs["input_features"].to(self.device)
        attention_mask = torch.ones(input_features.shape[:-1], dtype=torch.long).to(self.device)

        print("ƒêang nh·∫≠n di·ªán gi·ªçng n√≥i...")
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_features=input_features,
                attention_mask=attention_mask
            )

        return self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

# === H√†m main ===
def main():
    recorder = Recorder()
    model = AudioModel()
    model.load()

    while True:
        cmd = input("\nNh·∫≠p 'ok' ƒë·ªÉ ghi √¢m (ho·∫∑c 'q' ƒë·ªÉ tho√°t): ").strip().lower()
        if cmd == "ok":
            recorder.record_to_file()
            text = model.transcribe(recorder.save_path)
            print("\nK·∫øt qu·∫£ nh·∫≠n di·ªán:")
            print(text)
        elif cmd == "q":
            print("Tho√°t ch∆∞∆°ng tr√¨nh.")
            break
        else:
            print("Vui l√≤ng nh·∫≠p 'ok' ƒë·ªÉ ghi √¢m ho·∫∑c 'q' ƒë·ªÉ tho√°t.")

if __name__ == "__main__":
    main()
